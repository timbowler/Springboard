{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce using SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [SPARK](#SPARK)\n",
    "    * Installing Spark locally\n",
    "* [Spark Context](#Spark-Context)\n",
    "    * [Create A RDD](#Create-A-RDD)\n",
    "    * [Call `collect` on an RDD: Lazy Spark](#Call-collect-on-an-RDD:-Lazy-Spark)\n",
    "    * [Operations on RDDs](#Operations-on-RDDs)\n",
    "    * [Word Examples](#Word-Examples)\n",
    "    * [Key Value Pairs](#Key-Value-Pairs)\n",
    "    * [word count 1](#word-count-1)\n",
    "    * [word count 2:  `reduceByKey()`](#word-count-2:--reduceByKey%28%29)\n",
    "    * [Nested Syntax](#Nested-Syntax)\n",
    "    * [Using Cache](#Using-Cache)\n",
    "    * [Fun with words](#Fun-with-words)\n",
    "    * [DataFrames](#DataFrames)\n",
    "    * [Machine Learning](#Machine-Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With shameless stealing of some code and text from:\n",
    "\n",
    "- https://github.com/tdhopper/rta-pyspark-presentation/blob/master/slides.ipynb\n",
    "- Databricks and Berkeley Spark MOOC: https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\n",
    "\n",
    "which you should go check out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Spark locally\n",
    "\n",
    "\n",
    "**Step 1: Install Apache Spark**\n",
    "\n",
    "For example, for Mac users using Homebrew:\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Install the Java SDK version 1.8 or above for your platform (not just the JRE runtime)**\n",
    "\n",
    "Make sure you can access commands such as `java` on your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Install the latest findspark package using pip**\n",
    "\n",
    "```\n",
    "➜  ~  pip install findspark\n",
    "Collecting findspark\n",
    "  Downloading findspark-0.0.5-py2.py3-none-any.whl\n",
    "Installing collected packages: findspark\n",
    "Successfully installed findspark-0.0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Context\n",
    "\n",
    "You can also use it directly from the notebook interface on the mac if you installed `apache-spark` using `brew` and also installed `findspark` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('c:\\spark')\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dadpc.fios-router.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create A RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print(type(wordsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Call `collect` on an RDD: Lazy Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is lazy. Until you `collect`, nothing is actually run.\n",
    "\n",
    ">Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Spark Programming Guide:\n",
    "\n",
    ">RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    }
   ],
   "source": [
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "print(makePlural('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform one RDD into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "['cats', 'elephants']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print(pluralRDD.first())\n",
    "print(pluralRDD.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'elephants', 'rats', 'rats', 'cats']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda w: (w, 1))\n",
    "print(wordPairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WORD COUNT!\n",
    "\n",
    "This little exercise shows how to use mapreduce to calculate the counts of individual words in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda w: (w, 1))\n",
    "                       .reduceByKey(lambda x, y: x + y)\n",
    "                       .collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tons of shuffling](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[19] at RDD at PythonRDD.scala:48 []\\n |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:122 []\\n |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(4) PairwiseRDD[16] at reduceByKey at <ipython-input-13-ac2e2bf5d666>:1 []\\n    |  PythonRDD[15] at reduceByKey at <ipython-input-13-ac2e2bf5d666>:1 []\\n    |  ParallelCollectionRDD[9] at parallelize at PythonRDD.scala:175 []'\n"
     ]
    }
   ],
   "source": [
    "print(wordsRDD.map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y).toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[20] at parallelize at PythonRDD.scala:175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "print(wordsRDD)\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, every operation is run from the start. This may be inefficient in many cases. So when appropriate, we may want to cache the result the first time an operation is run on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is rerun from the start\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[20] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default storage level (MEMORY_ONLY)\n",
    "wordsRDD.cache()#nothing done this is still lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parallelize is rerun and cached because we told it to cache\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this `sc.parallelize` is not rerun in this case\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is this useful: it is when you have branching parts or loops, so that you dont do things again and again. Spark, being \"lazy\" will rerun the chain again. So `cache` or `persist` serves as a checkpoint, breaking the RDD chain or the *lineage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 'mammal',\n",
       " 'elephant': 'mammal',\n",
       " 'heron': 'bird',\n",
       " 'owl': 'bird',\n",
       " 'rat': 'mammal'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birdsList = ['heron','owl']\n",
    "animList = wordsList+birdsList\n",
    "animaldict = {}\n",
    "for e in wordsList:\n",
    "    animaldict[e] = 'mammal'\n",
    "for e in birdsList:\n",
    "    animaldict[e] = 'bird'\n",
    "animaldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2\n"
     ]
    }
   ],
   "source": [
    "animsrdd = sc.parallelize(animList, 4)\n",
    "animsrdd.cache()\n",
    "#below runs the whole chain but causes cache to be populated\n",
    "mammalcount = animsrdd.filter(lambda w: animaldict[w]=='mammal').count()\n",
    "#now only the filter is carried out\n",
    "birdcount = animsrdd.filter(lambda w: animaldict[w]=='bird').count()\n",
    "print(mammalcount, birdcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Fun with MapReduce\n",
    "\n",
    "Read http://spark.apache.org/docs/latest/programming-guide.html for some useful background and then try out the following exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `./sparklect/english.stop.txt` contains a list of English stopwords, while the file `./sparklect/shakes/juliuscaesar.txt` contains the entire text of Shakespeare's 'Julius Caesar'.\n",
    "\n",
    "* Load all of the stopwords into a Python list\n",
    "* Load the text of Julius Caesar into an RDD using the `sparkcontext.textFile()` method. Call it `juliusrdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn\n",
    "stopw = pd.read_csv('c:/users/tim/documents/springboard/spark_mini_project/sparklect/english.stop.txt')\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext()\n",
    "juliusrdd = sc.textFile('c:/users/tim/documents/springboard/spark_mini_project/sparklect/shakes/juliuscaesar.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words does Julius Caesar have? *Hint: use `flatMap()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn\n",
    "counts = juliusrdd.flatMap(lambda l: l.split(' ')).map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1599', 1)\n",
      "('TRAGEDY', 1)\n",
      "('OF', 16)\n",
      "('JULIUS', 2)\n",
      "('', 12571)\n",
      "('Shakespeare', 1)\n",
      "('Dramatis', 1)\n",
      "('Personae', 1)\n",
      "('statesman', 1)\n",
      "('OCTAVIUS,', 1)\n",
      "('after', 10)\n",
      "('death,', 6)\n",
      "('Caesar,', 45)\n",
      "('of', 352)\n",
      "('ANTONY,', 1)\n",
      "('his', 152)\n",
      "('third', 3)\n",
      "('MARCUS', 1)\n",
      "('BRUTUS,', 2)\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "for i in range(0, 19):\n",
    "    print(counts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar, **after removing all the stopwords**. *Hint: use `filter()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1599', 1)\n",
      "('tragedy', 1)\n",
      "('of', 388)\n",
      "('julius', 4)\n",
      "('caesar', 205)\n",
      "('shakespeare', 6)\n",
      "('dramatis', 1)\n",
      "('roman', 16)\n",
      "('statesman', 1)\n",
      "('triumvir', 2)\n",
      "('after', 11)\n",
      "(\"caesar's\", 41)\n",
      "('mark', 33)\n",
      "('his', 161)\n",
      "('lepidus', 11)\n",
      "('third', 20)\n",
      "('marcus', 7)\n",
      "('leader', 1)\n",
      "('against', 10)\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "def notStopWord(w):\n",
    "    l = []\n",
    "    l.append(w.encode('ascii', 'replace'))\n",
    "    if (w==u'') | (w==u'a'):   # removing newline; 'a' should be in the list of stop words \n",
    "        return False\n",
    "    return ~any(stopw.isin(l))\n",
    "\n",
    "counts = (juliusrdd\n",
    "          .flatMap(lambda l: l.lower().split(' '))\n",
    "          .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "          .filter(lambda w: notStopWord(w))\n",
    "          .map(lambda w: (w,1))\n",
    "          .reduceByKey(lambda a, b: a + b)\n",
    "          .collect())\n",
    "\n",
    "for i in range(0, 19):\n",
    "    print(counts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the word counting MapReduce code you've seen before. Count the number of times each word occurs and print the top 20 results as a list of tuples of the form `(word, count)`. *Hint: use `takeOrdered()` instead of `take()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 655), ('the', 612), ('i', 518), ('to', 418), ('of', 388), ('you', 378), ('brutus', 355), ('that', 285), ('is', 257), ('not', 251), ('in', 228), ('cassius', 218), ('caesar', 205), ('he', 190), ('my', 189), ('it', 186), ('for', 184), ('me', 177), ('with', 163), ('his', 161)]\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "counts = (juliusrdd\n",
    "          .flatMap(lambda l: l.lower().split(' '))\n",
    "          .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "          .filter(lambda w: notStopWord(w))\n",
    "          .map(lambda w: (w, 1))\n",
    "          .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "top20 = counts.takeOrdered(20, key=lambda x: -x[1])\n",
    "print(top20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a bar graph. For each of the top 20 words on the X axis, represent the count on the Y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d72fd3ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your turn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = zip(*top20)\n",
    "ax = sns.barplot(x, y)\n",
    "ax.set_title('Top 20 Words in Shakespeare\\'s Julius Caesar')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using partitions for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make your code more efficient, you want to use all of the available processing power, even on a single laptop. If your machine has multiple cores, you can tune the number of partitions to use all of them! From http://www.stat.berkeley.edu/scf/paciorek-spark-2014.html:\n",
    "\n",
    ">You want each partition to be able to fit in the memory availalbe on a node, and if you have multi-core nodes, you want that as many partitions as there are cores be able to fit in memory.\n",
    "\n",
    ">For load-balancing you'll want at least as many partitions as total computational cores in your cluster and probably rather more partitions. The Spark documentation suggests 2-4 partitions (which they also seem to call slices) per CPU. Often there are 100-10,000 partitions. Another rule of thumb is that tasks should take at least 100 ms. If less than that, you may want to repartition to have fewer partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakesrdd = sc.textFile('c:/users/tim/documents/springboard/spark_mini_project/sparklect/shakes/*.txt', minPartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1601',\n",
       " 'AS YOU LIKE IT',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'DRAMATIS PERSONAE.',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakesrdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the top 20 words in all of the files that you just read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 11383), ('and', 10778), ('i', 8611), ('to', 7776), ('of', 6563), ('you', 5899), ('my', 5024), ('that', 4506), ('in', 4463), ('is', 3810), ('not', 3529), ('with', 3265), ('for', 3195), ('it', 3132), ('me', 2909), ('your', 2901), ('be', 2781), ('this', 2628), ('he', 2580), ('his', 2574)]\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "counts = (shakesrdd\n",
    "          .flatMap(lambda l: l.lower().split(' '))\n",
    "          .map(lambda w: w.strip(\".\").strip(\",\").strip('\"').strip(\"'\"))\n",
    "          .filter(lambda w: notStopWord(w))\n",
    "          .map(lambda w: (w, 1))\n",
    "          .reduceByKey(lambda a, b: a + b))\n",
    "\n",
    "top20 = counts.takeOrdered(20, key=lambda x: -x[1])\n",
    "print(top20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 1: DataFrames\n",
    "\n",
    "Pandas and Spark dataframes can be easily converted to each other, making it easier to work with different data formats. This section shows some examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Spark DataFrame to Pandas\n",
    "\n",
    "`pandas_df = spark_df.toPandas()`\n",
    "\n",
    "Create a Spark DataFrame from Pandas\n",
    "\n",
    "`spark_df = context.createDataFrame(pandas_df)`\n",
    "\n",
    "Must fit in memory.\n",
    "\n",
    "![](https://ogirardot.files.wordpress.com/2015/05/rdd-vs-dataframe.png?w=640&h=360)\n",
    "\n",
    "VERY IMPORTANT: DataFrames in Spark are like RDD in the sense that they’re an immutable data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>73.847017</td>\n",
       "      <td>241.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>68.781904</td>\n",
       "      <td>162.310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>74.110105</td>\n",
       "      <td>212.740856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>71.730978</td>\n",
       "      <td>220.042470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>69.881796</td>\n",
       "      <td>206.349801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender     Height      Weight\n",
       "0   Male  73.847017  241.893563\n",
       "1   Male  68.781904  162.310473\n",
       "2   Male  74.110105  212.740856\n",
       "3   Male  71.730978  220.042470\n",
       "4   Male  69.881796  206.349801"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('c:/users/tim/documents/springboard/spark_mini_project/sparklect/01_heights_weights_genders.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this pandas dataframe to a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Height: double, Weight: double]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc = SQLContext(sc)\n",
    "sparkdf = sqlsc.createDataFrame(df)\n",
    "sparkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+\n",
      "|Gender|           Height|          Weight|\n",
      "+------+-----------------+----------------+\n",
      "|  Male|  73.847017017515|241.893563180437|\n",
      "|  Male|68.78190404589029|  162.3104725213|\n",
      "|  Male|74.11010539178491|  212.7408555565|\n",
      "|  Male| 71.7309784033377|220.042470303077|\n",
      "|  Male| 69.8817958611153|206.349800623871|\n",
      "+------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparkdf.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sparkdf.rdd.map(lambda r: r.Gender)\n",
    "print(type(temp))\n",
    "temp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 2: Machine Learning using Spark\n",
    "\n",
    "While we don't go in-depth into machine learning using spark here, this sample code will help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a data set from the Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [73.847017017515,241.893563180437]),\n",
       " LabeledPoint(1.0, [68.78190404589029,162.3104725213]),\n",
       " LabeledPoint(1.0, [74.11010539178491,212.7408555565]),\n",
       " LabeledPoint(1.0, [71.7309784033377,220.042470303077]),\n",
       " LabeledPoint(1.0, [69.8817958611153,206.349800623871])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sparkdf.rdd.map(lambda row: LabeledPoint(row.Gender=='Male', [row.Height, row.Weight]))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, DenseVector([73.847, 241.8936]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = sparkdf.rdd.map(lambda row: LabeledPoint(row[0]=='Male', row[1:]))\n",
    "data2.take(1)[0].label, data2.take(1)[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[42] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.7,0.3])\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the logistic regression model using MLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.4823, 0.1983])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 0.0), (1.0, 1.0), (1.0, 1.0), (1.0, 0.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = test.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "print(results.take(10))\n",
    "type(results)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9220736876426475"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = results.filter(lambda t: t[0]==t[1]).count()/float(results.count())\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.evaluation.BinaryClassificationMetrics'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.922075886991081"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(metrics))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mylogistic.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(sc, 'mylogistic.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline API automates a lot of this stuff, allowing us to work directly on dataframes. It is not all supported in Python, as yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see:\n",
    "\n",
    "- http://jordicasanellas.weebly.com/data-science-blog/machine-learning-with-spark\n",
    "- http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "- http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/\n",
    "- http://spark.apache.org/docs/latest/api/python/\n",
    "- http://spark.apache.org/docs/latest/programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.saveAsTextFile()` saves an RDD as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Topic 3: Your Turn at Machine Learning! :)\n",
    "\n",
    "For this exercise, we're going to use one of the datasets we've already worked with: the Boston House Prices dataset. We're going to try a couple of regression algorithms, but from the SparkML library this time.\n",
    "\n",
    "Before you proceed, make sure to do an overview of the documentation: \n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "import numpy as np\n",
    "import pyspark.ml.regression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the dataset, which resides as a CSV file in the folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path: /sparklect/boston.csv\n",
    "boston_df = pd.read_csv('c:/users/tim/documents/springboard/spark_mini_project/sparklect/boston.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data to make sure everything is loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio   black  lstat  medv\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3  396.90   4.98  24.0\n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8  396.90   9.14  21.6\n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8  392.83   4.03  34.7\n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7  394.63   2.94  33.4\n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7  396.90   5.33  36.2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to create a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[crim: double, zn: double, indus: double, chas: bigint, nox: double, rm: double, age: double, dis: double, rad: bigint, tax: bigint, ptratio: double, black: double, lstat: double, medv: double]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll first have to vectorize the features\n",
    "from pyspark.sql import SQLContext\n",
    "sqlsc = SQLContext(sc)\n",
    "bos_sparkdf = sqlsc.createDataFrame(boston_df)\n",
    "bos_sparkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+----+-------------------+-----------------+----+------+---+---+-------+------+-----+----+\n",
      "|                crim|  zn|indus|chas|                nox|               rm| age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+--------------------+----+-----+----+-------------------+-----------------+----+------+---+---+-------+------+-----+----+\n",
      "|             0.00632|18.0| 2.31|   0| 0.5379999999999999|            6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|             0.02731| 0.0| 7.07|   0|              0.469|            6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|             0.02729| 0.0| 7.07|   0|              0.469|            7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.032369999999999996| 0.0| 2.18|   0|0.45799999999999996|6.997999999999999|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|             0.06905| 0.0| 2.18|   0|0.45799999999999996|            7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "+--------------------+----+-----+----+-------------------+-----------------+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bos_sparkdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(24.0, [0.00632,18.0,2.31,0.0,0.5379999999999999,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]),\n",
       " LabeledPoint(21.6, [0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14]),\n",
       " LabeledPoint(34.7, [0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03]),\n",
       " LabeledPoint(33.4, [0.032369999999999996,0.0,2.18,0.0,0.45799999999999996,6.997999999999999,45.8,6.0622,3.0,222.0,18.7,394.63,2.94]),\n",
       " LabeledPoint(36.2, [0.06905,0.0,2.18,0.0,0.45799999999999996,7.147,54.2,6.0622,3.0,222.0,18.7,396.9,5.33])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD METHOD\n",
    "data2 = bos_sparkdf.rdd.map(lambda line:LabeledPoint(line[-1], [line[:-1]]))\n",
    "data2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|medv|\n",
      "+--------------------+----+\n",
      "|[0.00632,18.0,2.3...|24.0|\n",
      "|[0.02731,0.0,7.07...|21.6|\n",
      "|[0.02729,0.0,7.07...|34.7|\n",
      "+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NEW METHOD\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat'], \n",
    "    outputCol = 'features')\n",
    "vbos_sparkdf = vectorAssembler.transform(bos_sparkdf)\n",
    "vbos_sparkdf = vbos_sparkdf.select(['features', 'medv'])\n",
    "vbos_sparkdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = vbos_sparkdf.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT SPARK DF TO NUMP ARRAYS (JUST TO DEMO)\n",
    "data3 = np.array(bos_sparkdf.select('crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio',\n",
    "                                    'black', 'lstat', 'medv').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00, 2.4000e+01],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00, 2.1600e+01],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9283e+02, 4.0300e+00, 3.4700e+01],\n",
       "       [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
       "        6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
       "        1.8700e+01, 3.9463e+02, 2.9400e+00, 3.3400e+01],\n",
       "       [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
       "        7.1470e+00, 5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
       "        1.8700e+01, 3.9690e+02, 5.3300e+00, 3.6200e+01]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, fit a Linear Regression model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,1.0200395192849547,-4.907103576744952,4.4489693629139975,0.0,-0.6247590560438953,0.0,-0.002065140762955304,-0.8742364062824205,0.007010103665467262,-0.5164856293792472]\n",
      "Intercept: 20.773448862277743\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='medv', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train)\n",
    "print('Coefficients: ' + str(lr_model.coefficients))\n",
    "print('Intercept: ' + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now validate the model on the test set, and check the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared on test data = 0.672473\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='medv', metricName='r2')\n",
    "print('R Squared on test data = %g' % evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[0.0136,75.0,4.0,...|18.9| 16.10740278175141|\n",
      "|[0.02985,0.0,2.18...|28.7|   26.610703958827|\n",
      "|[0.03358999999999...|34.9|31.780137157668356|\n",
      "|[0.03932,0.0,3.41...|22.0|27.342354604330087|\n",
      "|[0.05058999999999...|23.9|25.138906289041458|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 5.08495\n"
     ]
    }
   ],
   "source": [
    "print('RMSE on test data = %g' % lr_model.evaluate(test).rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Linear Regression with a more powerful algorithm - the Random Forest. As the Random Forest has several hyperparameters that can be tuned for maximum accuracy, we're going to need to use k-fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up a grid for the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='medv')\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start=10, stop=110, num=11)]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start=200, stop=2000, num=10)]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a Random Forest regressor using k-fold Cross Validation, and find the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                          evaluator=RegressionEvaluator(predictionCol='prediction', labelCol='medv', metricName='rmse'), \n",
    "                          numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0063, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.09, 1.0, 296.0, 15.3, 396.9, 4.98]), medv=24.0),\n",
       " Row(features=DenseVector([0.0131, 90.0, 1.22, 0.0, 0.403, 7.249, 21.9, 8.6966, 5.0, 226.0, 17.9, 395.93, 4.81]), medv=35.4),\n",
       " Row(features=DenseVector([0.0143, 100.0, 1.32, 0.0, 0.411, 6.816, 40.5, 8.3248, 5.0, 256.0, 15.1, 392.9, 3.95]), medv=31.6),\n",
       " Row(features=DenseVector([0.0195, 17.5, 1.38, 0.0, 0.4161, 7.104, 59.5, 9.2229, 3.0, 216.0, 18.6, 393.24, 8.05]), medv=33.0),\n",
       " Row(features=DenseVector([0.0206, 85.0, 0.74, 0.0, 0.41, 6.383, 35.7, 9.1876, 2.0, 313.0, 17.3, 396.9, 5.77]), medv=24.7)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|medv|\n",
      "+--------------------+----+\n",
      "|[0.00632,18.0,2.3...|24.0|\n",
      "|[0.01311,90.0,1.2...|35.4|\n",
      "|[0.01432,100.0,1....|31.6|\n",
      "|[0.01951,17.5,1.3...|33.0|\n",
      "|[0.02055,85.0,0.7...|24.7|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS DOESN'T WORK YET.  TRYING TO FIGURE IT OUT\n",
    "# rfModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate the model on the test set and check the Root Mean Squared Error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
